{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLab_v3_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N7LS-HGQVQou",
        "E4pFO0OjUTQB",
        "aE4DxXTPizUV"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN863FEpPcoW6AEq/XrWDzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerogar/DeepLabV3p-Pytorch-Tutorial_KOR/blob/master/DeepLab_v3_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi0egK65w35f"
      },
      "source": [
        "# DeepLab v3 Tutorial\r\n",
        "\r\n",
        "가짜연구소 Season 2 논문미식회\r\n",
        "\r\n",
        "reference: https://github.com/VainF/DeepLabV3Plus-Pytorch\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "![Main](https://sthalles.github.io/assets/deep_segmentation_network/semantic_segmentation.jpg)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MfKqK5A4v5Z"
      },
      "source": [
        "# 1. 환경 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1cnSVwXZvj"
      },
      "source": [
        "## (1) Mount Google Drive\r\n",
        "- 구글 드라이브를 연결하고 작업 폴더를 Colab Notebooks/로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeGYqxh24oZ-",
        "outputId": "1d3d8e69-8d45-4eab-d810-2e380239d9d7"
      },
      "source": [
        "from google.colab import drive # import drive from google colab\r\n",
        "\r\n",
        "ROOT = \"/content/drive\"     # default location for the drive\r\n",
        "print(ROOT)                 # print content of ROOT (Optional)\r\n",
        "\r\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive\r\n",
        "\r\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\r\n",
        "from os.path import join  \r\n",
        "\r\n",
        "# path to your project on Google Drive\r\n",
        "MY_GOOGLE_DRIVE_PATH = '/content/drive/My Drive/Colab Notebooks'\r\n",
        "\r\n",
        "print(\"MY_GOOGLE_DRIVE_PATH: \", MY_GOOGLE_DRIVE_PATH)\r\n",
        "# In case we haven't created the folder already; we will create a folder in the project path  \r\n",
        "%cd \"{MY_GOOGLE_DRIVE_PATH}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Mounted at /content/drive\n",
            "MY_GOOGLE_DRIVE_PATH:  /content/drive/My Drive/Colab Notebooks\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLRclgki42mb"
      },
      "source": [
        "## (2) Set Work directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6ohDpwJ95R7",
        "outputId": "e8b43c3b-82fe-4163-986c-625e9196fafe"
      },
      "source": [
        "%cd DeepLabV3Plus-Pytorch "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DeepLabV3Plus-Pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOi091h8cAbI",
        "outputId": "f9d3cc68-7f90-4990-8fb6-201eadad0105"
      },
      "source": [
        "!pip install torch torchvision numpy pillow scikit-learn tqdm matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Sbl6YTaxtp"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from torch.utils import data\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "import torch.nn.functional as F\r\n",
        "from collections import OrderedDict\r\n",
        "\r\n",
        "from torchvision.transforms.functional import normalize\r\n",
        "from sklearn.metrics import confusion_matrix\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrHnDiOzJ2CB"
      },
      "source": [
        "#2. DeepLab v3 Model\r\n",
        "\r\n",
        "- DeepLab v1 - atrous convolution 및 Conditional Random Field(CRF) 도입\r\n",
        "- DeepLab v2 - A*trous spatial pyramid pooling* (ASPP) 이용하여 다양한 크기의 객체에 대응\r\n",
        "- DeepLab v3 - encoder에 ResNet 구조 도입. ASPP를 보완하고, batch norm. 사용하여 학습이 잘 되도록 함.  CRF 없이 동등 이상 성능 확보\r\n",
        "- DeepLab v3+: Depth-wise seperable Conv.도입하여 런타임 속도 개선. (ASSPP) U-Net 구조를 단순화한 Decoder 구조 사용하여 성능 개선\r\n",
        "\r\n",
        "![deeplab](https://miro.medium.com/max/1038/0*_Hm_2fqbcnlwLkoz.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zacdJ4PLmAM-"
      },
      "source": [
        "## (1) Overall Structure\r\n",
        "![deeplabv3](https://www.oreilly.com/library/view/hands-on-image-processing/9781789343731/assets/1aa5b349-5a66-456a-8afa-080a7b07a525.png)\r\n",
        "\r\n",
        "- Backbone Network + DeepLab Head 구조\r\n",
        "- Resnet의 layer 4의 feature를 backbone feature로 사용\r\n",
        "- output_stride를 8로 설정하는 경우 \r\n",
        "    - Backborn의 stride=8에 해당하는 layer까지 사용\r\n",
        "    - ResNet을 사용하는 경우 layer2까지 사용하며, stride를 8로 유지하며 layer 3, 4의 atrous  convolution으로 대체.\r\n",
        "    - DeepLab Head의 Atrous Spatial Pyramid Pooling(ASPP)의 dilation rate=[12, 24, 36] 사용\r\n",
        "- output_stride를 16로 설정하는 경우 \r\n",
        "    - Backborn의 stride=16에 해당하는 layer까지 사용\r\n",
        "    - ResNet을 사용하는 경우 layer3까지 사용하며, stride를 16로 유지하며 layer 4를 atrous  convolution으로 대체.\r\n",
        "    - DeepLab Head의 Atrous Spatial Pyramid Pooling(ASPP)의 dilation rate=[6, 12, 18] 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzYYks05t9D"
      },
      "source": [
        "from collections import OrderedDict\r\n",
        "\r\n",
        "def ResNetbasedDeepLabV3(num_classes, output_stride, pretrained_backbone):\r\n",
        "    if output_stride==8:\r\n",
        "        replace_stride_with_dilation=[False, True, True]\r\n",
        "        aspp_dilate = [12, 24, 36]\r\n",
        "    else:\r\n",
        "        replace_stride_with_dilation=[False, False, True]\r\n",
        "        aspp_dilate = [6, 12, 18]\r\n",
        "        \r\n",
        "    backbone = resnet50(\r\n",
        "        pretrained=pretrained_backbone,\r\n",
        "        replace_stride_with_dilation=replace_stride_with_dilation)\r\n",
        "    \r\n",
        "    inplanes = 2048\r\n",
        "    low_level_planes = 256\r\n",
        "\r\n",
        "    return_layers = {'layer4': 'out'}\r\n",
        "    classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\r\n",
        "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\r\n",
        "\r\n",
        "\r\n",
        "    # TODO: interpolation parameter 추가\r\n",
        "    model = DeepLabV3(backbone, classifier, 'CARAFE')\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt77SLuOKuI_"
      },
      "source": [
        "##(2) Backborn - ResNet50 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yY7qHJ7LKib"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision.models.utils import load_state_dict_from_url\r\n",
        "\r\n",
        "model_urls = {\r\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\r\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\r\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\r\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\r\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\r\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\r\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\r\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\r\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\r\n",
        "}\r\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\r\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\r\n",
        "def conv1x1(in_planes, out_planes, stride=1):\r\n",
        "    \"\"\"1x1 convolution\"\"\"\r\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\r\n",
        "class BasicBlock(nn.Module):\r\n",
        "    expansion = 1\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                 base_width=64, dilation=1, norm_layer=None):\r\n",
        "        super(BasicBlock, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        if groups != 1 or base_width != 64:\r\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\r\n",
        "        if dilation > 1:\r\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\r\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\r\n",
        "        self.bn1 = norm_layer(planes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.conv2 = conv3x3(planes, planes)\r\n",
        "        self.bn2 = norm_layer(planes)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "class Bottleneck(nn.Module):\r\n",
        "    expansion = 4\r\n",
        "\r\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\r\n",
        "                 base_width=64, dilation=1, norm_layer=None):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        width = int(planes * (base_width / 64.)) * groups\r\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\r\n",
        "        self.conv1 = conv1x1(inplanes, width)\r\n",
        "        self.bn1 = norm_layer(width)\r\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\r\n",
        "        self.bn2 = norm_layer(width)\r\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\r\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.downsample = downsample\r\n",
        "        self.stride = stride\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        identity = x\r\n",
        "\r\n",
        "        out = self.conv1(x)\r\n",
        "        out = self.bn1(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv2(out)\r\n",
        "        out = self.bn2(out)\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        out = self.conv3(out)\r\n",
        "        out = self.bn3(out)\r\n",
        "\r\n",
        "        if self.downsample is not None:\r\n",
        "            identity = self.downsample(x)\r\n",
        "\r\n",
        "        out += identity\r\n",
        "        out = self.relu(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "class ResNet(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\r\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\r\n",
        "                 norm_layer=None):\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        if norm_layer is None:\r\n",
        "            norm_layer = nn.BatchNorm2d\r\n",
        "        self._norm_layer = norm_layer\r\n",
        "\r\n",
        "        self.inplanes = 64\r\n",
        "        self.dilation = 1\r\n",
        "        if replace_stride_with_dilation is None:\r\n",
        "            # each element in the tuple indicates if we should replace\r\n",
        "            # the 2x2 stride with a dilated convolution instead\r\n",
        "            replace_stride_with_dilation = [False, False, False]\r\n",
        "        if len(replace_stride_with_dilation) != 3:\r\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\r\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\r\n",
        "        self.groups = groups\r\n",
        "        self.base_width = width_per_group\r\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\r\n",
        "                               bias=False)\r\n",
        "        self.bn1 = norm_layer(self.inplanes)\r\n",
        "        self.relu = nn.ReLU(inplace=True)\r\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\r\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[0])\r\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[1])\r\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\r\n",
        "                                       dilate=replace_stride_with_dilation[2])\r\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\r\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n",
        "\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\r\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "        # Zero-initialize the last BN in each residual branch,\r\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\r\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\r\n",
        "        if zero_init_residual:\r\n",
        "            for m in self.modules():\r\n",
        "                if isinstance(m, Bottleneck):\r\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\r\n",
        "                elif isinstance(m, BasicBlock):\r\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\r\n",
        "\r\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\r\n",
        "        norm_layer = self._norm_layer\r\n",
        "        downsample = None\r\n",
        "        previous_dilation = self.dilation\r\n",
        "        if dilate:\r\n",
        "            self.dilation *= stride\r\n",
        "            stride = 1\r\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\r\n",
        "            downsample = nn.Sequential(\r\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\r\n",
        "                norm_layer(planes * block.expansion),\r\n",
        "            )\r\n",
        "\r\n",
        "        layers = []\r\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\r\n",
        "                            self.base_width, previous_dilation, norm_layer))\r\n",
        "        self.inplanes = planes * block.expansion\r\n",
        "        for _ in range(1, blocks):\r\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\r\n",
        "                                base_width=self.base_width, dilation=self.dilation,\r\n",
        "                                norm_layer=norm_layer))\r\n",
        "\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        x = self.maxpool(x)\r\n",
        "\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.layer4(x)\r\n",
        "\r\n",
        "        x = self.avgpool(x)\r\n",
        "        x = torch.flatten(x, 1)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\r\n",
        "    model = ResNet(block, layers, **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\r\n",
        "                                              progress=progress)\r\n",
        "        model.load_state_dict(state_dict)\r\n",
        "    return model\r\n",
        "\r\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\r\n",
        "    \"\"\"ResNet-50 model from <https://arxiv.org/pdf/1512.03385.pdf>`_\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "    \"\"\"\r\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\r\n",
        "                   **kwargs)\r\n",
        "\r\n",
        "class IntermediateLayerGetter(nn.ModuleDict):\r\n",
        "    '''\r\n",
        "    model의 중간 layer를 return하는 class\r\n",
        "    사용되는 순서대로 layer가 등록?되었다는 가정하에 동작함. \r\n",
        "\r\n",
        "    Arguments:\r\n",
        "        model (nn.Module): model on which we will extract the features\r\n",
        "        return_layers (Dict[name, new_name]): 추출하고자 하는 layer명과 사용할 변수명\r\n",
        "    Examples::\r\n",
        "        >>> m = torchvision.models.resnet18(pretrained=True)\r\n",
        "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\r\n",
        "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\r\n",
        "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\r\n",
        "        >>> out = new_m(torch.rand(1, 3, 224, 224))\r\n",
        "        >>> print([(k, v.shape) for k, v in out.items()])\r\n",
        "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\r\n",
        "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\r\n",
        "    '''\r\n",
        "    def __init__(self, model, return_layers):\r\n",
        "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\r\n",
        "            raise ValueError(\"return_layers are not present in model\")\r\n",
        "\r\n",
        "        orig_return_layers = return_layers\r\n",
        "        return_layers = {k: v for k, v in return_layers.items()}\r\n",
        "        layers = OrderedDict()\r\n",
        "        for name, module in model.named_children():\r\n",
        "            layers[name] = module\r\n",
        "            if name in return_layers:\r\n",
        "                del return_layers[name]\r\n",
        "            if not return_layers:\r\n",
        "                break\r\n",
        "\r\n",
        "        super(IntermediateLayerGetter, self).__init__(layers)\r\n",
        "        self.return_layers = orig_return_layers\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = OrderedDict()\r\n",
        "        for name, module in self.named_children():\r\n",
        "            x = module(x)\r\n",
        "            if name in self.return_layers:\r\n",
        "                out_name = self.return_layers[name]\r\n",
        "                out[out_name] = x\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o63GzStmvhK"
      },
      "source": [
        "##(3) DeepLab Head\r\n",
        "- ASPP(2048ch→ 256ch) + 3x3 Conv(256ch→256ch) + 1x1 Conv(256ch→21:number of class)\r\n",
        "\r\n",
        "- Backbone + DeepLab v3 Head를 거쳐 도출된 segmentation 결과를, Bilinear upsampling을 이용하여 image-level 해상도로 복원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9vmrVu7nv2W"
      },
      "source": [
        "class DeepLabV3(nn.Module):\r\n",
        "\r\n",
        "    # TODO: interpolation parameter 추가\r\n",
        "    #def __init__(self, backbone, classifier):\r\n",
        "    def __init__(self, backbone, classifier, upMethod = ''):\r\n",
        "        super(DeepLabV3, self).__init__()\r\n",
        "        self.backbone = backbone\r\n",
        "        self.classifier = classifier\r\n",
        "        self.upsampleMethod = upMethod\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        input_shape = x.shape[-2:]\r\n",
        "        input_ch = x.shape[1] # 256\r\n",
        "        features = self.backbone(x)\r\n",
        "        x = self.classifier(features)\r\n",
        "\r\n",
        "        if self.upsampleMethod == 'CARAFE':\r\n",
        "            # upScale =input_shape[1] / x.shape[-1]\r\n",
        "            # print(input_ch)\r\n",
        "\r\n",
        "            carafe =CARAFE(c=21, scale=2)\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                carafe.cuda()\r\n",
        "            x = carafe(x) \r\n",
        "            \r\n",
        "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n",
        "\r\n",
        "        else :\r\n",
        "            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\r\n",
        "        \r\n",
        "        return x\r\n",
        "\r\n",
        "class DeepLabHead(nn.Module):\r\n",
        "    def __init__(self, in_channels, num_classes, aspp_dilate=[12, 24, 36]):\r\n",
        "        super(DeepLabHead, self).__init__()\r\n",
        "\r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            ASPP(in_channels, aspp_dilate),\r\n",
        "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\r\n",
        "            nn.BatchNorm2d(256),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(256, num_classes, 1)\r\n",
        "        )\r\n",
        "        self._init_weight()\r\n",
        "\r\n",
        "    def forward(self, feature):\r\n",
        "        return self.classifier( feature['out'] )\r\n",
        "\r\n",
        "    def _init_weight(self):\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight)\r\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7LS-HGQVQou"
      },
      "source": [
        "### ASPP (Atrous Spatial Pyramid Pooling)\r\n",
        "- 초기 Object Detection에 적용된 SPPNet의 Idea 차용 Atrous Conv.로 구성한 SPP 구성\r\n",
        "\r\n",
        "    → ConvLayer의 output을 다양한 Atrous rate를 가지는 kernel을 병렬로 연산하여 multi-scale 특징을 추출하고자 함.\r\n",
        "\r\n",
        "- Atrous rate가 커질수록 유효한 weight의 수가 작아지는 경향을 보임.\r\n",
        "    ⇒  Atorus rate가 커지면 3x3 kernel이 1x1처럼 동작함.\r\n",
        "\r\n",
        "    ⇒ large scale context는 output에 반영 안됨. (receptive field를 크게 만들고 싶은데 안됨..)\r\n",
        "\r\n",
        "- 이런 degenerate 문제 해결을 위해 이미지의 last feature map에 대해서 \"Global average pooling\"을 적용.\r\n",
        "- 성능 개선을 위해 각 module에 batch normalization 적용\r\n",
        "\r\n",
        "![ASPP](https://gaussian37.github.io/assets/img/vision/segmentation/aspp/0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4pFO0OjUTQB"
      },
      "source": [
        "### Atrous Convolution (=Dilated Convolution)\r\n",
        "\r\n",
        "- Kernel의 성분들 사이에 빈 성분(0)을 삽입하여 convolution을 수행\r\n",
        "- Dilation rate는 빈 성분(0)을 몇 개 삽입할지 결정. 기존 일반적인 convolution의 dilation rate=1. dilation rate=2일 경우는 성분들 사이에 빈 성분(0) 1개 추가, 3일 경우는 2개 추가, ...\r\n",
        "\r\n",
        "- 기존 3x3 Kernel에 아래와 같이 dilation rate=2를 사용하면 field-of-view(receptive field)가 5x5의 영역을 커버하게 됨. \r\n",
        "\r\n",
        "- x: input feature map, w: filter, r: dilation rate, y: output일 때 다음과 같이 표현 가능하다. \r\n",
        "$$y[i]=\\sum_{k}^{K}x[i+r\\cdot{k}]w[k]$$\r\n",
        "\r\n",
        "![https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif](https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif)\r\n",
        "*convolution*\r\n",
        "|\r\n",
        "![https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif](https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif)*Atrous Conv.*\r\n",
        "\r\n",
        "- **Atrous convolution은 pooling 연산 없이 넓은** field of view를 커버할 수 있음.\r\n",
        "(기존 CNN의 경우 넓은 field of view를 커버하기 위해 conv. pooling을 사용하여 output feature map의 해상도가 감소.)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48poPshQUSPz"
      },
      "source": [
        "class ASPPConv(nn.Sequential):\r\n",
        "    def __init__(self, in_channels, out_channels, dilation):\r\n",
        "        modules = [\r\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\r\n",
        "            nn.BatchNorm2d(out_channels),\r\n",
        "            nn.ReLU(inplace=True)\r\n",
        "        ]\r\n",
        "        super(ASPPConv, self).__init__(*modules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjdyGj9tncj-"
      },
      "source": [
        "- Global Average Pooling + 1x1 Conv + Upsampling\r\n",
        "    - Global Average Pooling (nn.AdaptiveAvgPool2d): Channel별로 average를 구하여 pooling (ex. 2048x33x33 → 2048x1x1)\r\n",
        "    - 1x1 Conv: Output channel 수를 256으로  (ex. 2048x1x1 → 256x1x1)\r\n",
        "    - Upsampling: Size를 원래대로 (ex. 256x1x1 → 256x33x33)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcFlb6_gnNnM"
      },
      "source": [
        "class ASPPPooling(nn.Sequential):\r\n",
        "    def __init__(self, in_channels, out_channels):\r\n",
        "        super(ASPPPooling, self).__init__(\r\n",
        "            nn.AdaptiveAvgPool2d(1),\r\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n",
        "            nn.BatchNorm2d(out_channels),\r\n",
        "            nn.ReLU(inplace=True))\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        size = x.shape[-2:]\r\n",
        "        x = super(ASPPPooling, self).forward(x)\r\n",
        "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvqtb5ZXnTpE"
      },
      "source": [
        "- 병렬화된 1x1 Conv, 3개의 3x3 Dilated Conv, Image pooling(code의 ASPP Pooling)를 concatenate\r\n",
        "- Concatenate된 feature를 1x1 Conv를 통해 output channel을 256으로 생성 (5x256→256)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pumb6ECinPG_"
      },
      "source": [
        "class ASPP(nn.Module):\r\n",
        "    def __init__(self, in_channels, atrous_rates):\r\n",
        "        super(ASPP, self).__init__()\r\n",
        "        out_channels = 256\r\n",
        "        modules = []\r\n",
        "        modules.append(nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n",
        "            nn.BatchNorm2d(out_channels),\r\n",
        "            nn.ReLU(inplace=True)))\r\n",
        "\r\n",
        "        rate1, rate2, rate3 = tuple(atrous_rates)\r\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate1))\r\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate2))\r\n",
        "        modules.append(ASPPConv(in_channels, out_channels, rate3))\r\n",
        "        modules.append(ASPPPooling(in_channels, out_channels))\r\n",
        "\r\n",
        "        self.convs = nn.ModuleList(modules)\r\n",
        "\r\n",
        "        self.project = nn.Sequential(\r\n",
        "            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\r\n",
        "            nn.BatchNorm2d(out_channels),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Dropout(0.1),)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        res = []\r\n",
        "        for conv in self.convs:\r\n",
        "            res.append(conv(x))\r\n",
        "        res = torch.cat(res, dim=1)\r\n",
        "        return self.project(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYfdrVT43EPS"
      },
      "source": [
        "##(4) CARAFE\n",
        "Code from: https://github.com/XiaLiPKU/CARAFE/blob/master/carafe.py#L68\n",
        "\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnk8vvQN4xVI"
      },
      "source": [
        "class ConvBNReLU(nn.Module):\n",
        "    '''Module for the Conv-BN-ReLU tuple.'''\n",
        "    def __init__(self, c_in, c_out, kernel_size, stride, padding, dilation,\n",
        "                 use_relu=True):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "                c_in, c_out, kernel_size=kernel_size, stride=stride, \n",
        "                padding=padding, dilation=dilation, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c_out)\n",
        "        if use_relu:\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "        else:\n",
        "            self.relu = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CARAFE(nn.Module):\n",
        "    def __init__(self, c, c_mid=64, scale=2, k_up=5, k_enc=3):\n",
        "        \"\"\" The unofficial implementation of the CARAFE module.\n",
        "        The details are in \"https://arxiv.org/abs/1905.02188\".\n",
        "        Args:\n",
        "            c: The channel number of the input and the output.\n",
        "            c_mid: The channel number after compression.\n",
        "            scale: The expected upsample scale.\n",
        "            k_up: The size of the reassembly kernel.\n",
        "            k_enc: The kernel size of the encoder.\n",
        "        Returns:\n",
        "            X: The upsampled feature map.\n",
        "        \"\"\"\n",
        "        super(CARAFE, self).__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "        self.comp = ConvBNReLU(c, c_mid, kernel_size=1, stride=1, \n",
        "                               padding=0, dilation=1)\n",
        "        self.enc = ConvBNReLU(c_mid, (scale*k_up)**2, kernel_size=k_enc, \n",
        "                              stride=1, padding=k_enc//2, dilation=1, \n",
        "                              use_relu=False)\n",
        "        self.pix_shf = nn.PixelShuffle(scale)\n",
        "\n",
        "        self.upsmp = nn.Upsample(scale_factor=scale, mode='nearest')\n",
        "        self.unfold = nn.Unfold(kernel_size=k_up, dilation=scale, \n",
        "                                padding=k_up//2*scale)\n",
        "\n",
        "    def forward(self, X):\n",
        "        b, c, h, w = X.size()\n",
        "        h_, w_ = h * self.scale, w * self.scale\n",
        "        \n",
        "        W = self.comp(X)                                # b * m * h * w\n",
        "        W = self.enc(W)                                 # b * 100(sigma^2, k_up^2) * h * w\n",
        "        W = self.pix_shf(W)                             # b * 25(k_up^2) * h_ * w_\n",
        "        W = F.softmax(W, dim=1)                         # b * 25 * h_ * w_\n",
        "\n",
        "        X = self.upsmp(X)                               # b * c * h_ * w_\n",
        "        X = self.unfold(X)                              # b * 25c * h_ * w_\n",
        "        X = X.view(b, c, -1, h_, w_)                    # b * 25 * c * h_ * w_\n",
        "\n",
        "        X = torch.einsum('bkhw,bckhw->bchw', [W, X])    # b * c * h_ * w_\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3vgfeEzqEeC"
      },
      "source": [
        "# 3. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMUH2XvqcTJ8"
      },
      "source": [
        "###(1) PASCAL VOC 2012 Segmentation Task Dataset\r\n",
        "\r\n",
        "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\r\n",
        "https://academictorrents.com/details/df0aad374e63b3214ef9e92e178580ce27570e59"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJFZZQFc2yDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "8467195c-fcd1-419c-8e24-c57600e69e62"
      },
      "source": [
        "'''\r\n",
        "!wget -P datasets/data/ \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\r\n",
        "!tar -xvf datasets/data/VOCtrainval_11-May-2012.tar -C ./datasets/data\r\n",
        "\r\n",
        "# SegmentationClassAug.zip를 다운. (https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0)\r\n",
        "!wget -P datasets/data/ \"https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=1\"\r\n",
        "!unzip datasets/data/SegmentationClassAug.zip -d datasets/data/VOCdevkit/VOC2012/\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!wget -P datasets/data/ \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\\n!tar -xvf datasets/data/VOCtrainval_11-May-2012.tar -C ./datasets/data\\n\\n# SegmentationClassAug.zip를 다운. (https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=0)\\n!wget -P datasets/data/ \"https://www.dropbox.com/s/oeu149j8qtbs1x0/SegmentationClassAug.zip?dl=1\"\\n!unzip datasets/data/SegmentationClassAug.zip -d datasets/data/VOCdevkit/VOC2012/\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htMjRT9LccCX"
      },
      "source": [
        "###(2) Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7MlH7HVl34i"
      },
      "source": [
        "- Scaling, Crop, 상하반전으로 Training data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJVBSaepvKb_"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import collections\n",
        "import torch.utils.data as data\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def voc_cmap(N=256, normalized=False):\n",
        "    def bitget(byteval, idx):\n",
        "        return ((byteval & (1 << idx)) != 0)\n",
        "\n",
        "    dtype = 'float32' if normalized else 'uint8'\n",
        "    cmap = np.zeros((N, 3), dtype=dtype)\n",
        "    for i in range(N):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r = r | (bitget(c, 0) << 7-j)\n",
        "            g = g | (bitget(c, 1) << 7-j)\n",
        "            b = b | (bitget(c, 2) << 7-j)\n",
        "            c = c >> 3\n",
        "\n",
        "        cmap[i] = np.array([r, g, b])\n",
        "\n",
        "    cmap = cmap/255 if normalized else cmap\n",
        "    return cmap\n",
        "\n",
        "class VOCSegmentation(data.Dataset):\n",
        "    \"\"\"`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Segmentation Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of the VOC Dataset.\n",
        "        year (string, optional): The dataset year, supports years 2007 to 2012.\n",
        "        image_set (string, optional): Select the image_set to use, ``train``, ``trainval`` or ``val``\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    \"\"\"\n",
        "    cmap = voc_cmap()\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 year='2012',\n",
        "                 image_set='train',\n",
        "                 #download=False,\n",
        "                 transform=None):\n",
        "\n",
        "        is_aug=False\n",
        "        if year=='2012_aug':\n",
        "            is_aug = True\n",
        "            year = '2012'\n",
        "        \n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.year = year\n",
        "        #self.url = DATASET_YEAR_DICT[year]['url']\n",
        "        #self.filename = DATASET_YEAR_DICT[year]['filename']\n",
        "        #self.md5 = DATASET_YEAR_DICT[year]['md5']\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.image_set = image_set\n",
        "        base_dir = 'VOCdevkit/VOC2012'\n",
        "        voc_root = os.path.join(self.root, base_dir)\n",
        "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
        "\n",
        "\n",
        "        if not os.path.isdir(voc_root):\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "        \n",
        "        if is_aug and image_set=='train':\n",
        "            mask_dir = os.path.join(voc_root, 'SegmentationClassAug')\n",
        "            assert os.path.exists(mask_dir), \"SegmentationClassAug not found, please refer to README.md and prepare it manually\"\n",
        "            split_f = os.path.join( self.root, 'train_aug.txt')#'./datasets/data/train_aug.txt'\n",
        "        else:\n",
        "            mask_dir = os.path.join(voc_root, 'SegmentationClass')\n",
        "            splits_dir = os.path.join(voc_root, 'ImageSets/Segmentation')\n",
        "            split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
        "\n",
        "        if not os.path.exists(split_f):\n",
        "            raise ValueError(\n",
        "                'Wrong image_set entered! Please use image_set=\"train\" '\n",
        "                'or image_set=\"trainval\" or image_set=\"val\"')\n",
        "\n",
        "        with open(os.path.join(split_f), \"r\") as f:\n",
        "            file_names = [x.strip() for x in f.readlines()]\n",
        "        \n",
        "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "        self.masks = [os.path.join(mask_dir, x + \".png\") for x in file_names]\n",
        "        assert (len(self.images) == len(self.masks))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is the image segmentation.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        target = Image.open(self.masks[index])\n",
        "        if self.transform is not None:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    @classmethod\n",
        "    def decode_target(cls, mask):\n",
        "        \"\"\"decode semantic mask to RGB image\"\"\"\n",
        "        return cls.cmap[mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIWEGt3t3DP_"
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms.functional as torchvisionF\n",
        "import random \n",
        "import numbers\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#\n",
        "#  Extended Transforms for Semantic Segmentation\n",
        "#\n",
        "class ExtRandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
        "    Args:\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be flipped.\n",
        "        Returns:\n",
        "            PIL Image: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if random.random() < self.p:\n",
        "            return torchvisionF.hflip(img), torchvisionF.hflip(lbl)\n",
        "        return img, lbl\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "class ExtCompose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "    Args:\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
        "    Example:\n",
        "        >>> transforms.Compose([\n",
        "        >>>     transforms.CenterCrop(10),\n",
        "        >>>     transforms.ToTensor(),\n",
        "        >>> ])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        for t in self.transforms:\n",
        "            img, lbl = t(img, lbl)\n",
        "        return img, lbl\n",
        "\n",
        "    def __repr__(self):\n",
        "        format_string = self.__class__.__name__ + '('\n",
        "        for t in self.transforms:\n",
        "            format_string += '\\n'\n",
        "            format_string += '    {0}'.format(t)\n",
        "        format_string += '\\n)'\n",
        "        return format_string\n",
        "\n",
        "class ExtCenterCrop(object):\n",
        "    \"\"\"Crops the given PIL Image at the center.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "        Returns:\n",
        "            PIL Image: Cropped image.\n",
        "        \"\"\"\n",
        "        return center_crop(img, self.size), F.center_crop(lbl, self.size)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
        "\n",
        "class ExtRandomScale(object):\n",
        "    def __init__(self, scale_range, interpolation=Image.BILINEAR):\n",
        "        self.scale_range = scale_range\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be scaled.\n",
        "            lbl (PIL Image): Label to be scaled.\n",
        "        Returns:\n",
        "            PIL Image: Rescaled image.\n",
        "            PIL Image: Rescaled label.\n",
        "        \"\"\"\n",
        "        assert img.size == lbl.size\n",
        "        scale = random.uniform(self.scale_range[0], self.scale_range[1])\n",
        "        target_size = ( int(img.size[1]*scale), int(img.size[0]*scale) )\n",
        "        return torchvisionF.resize(img, target_size, self.interpolation), torchvisionF.resize(lbl, target_size, Image.NEAREST)\n",
        "\n",
        "    def __repr__(self):\n",
        "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
        "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
        "\n",
        "class ExtRandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
        "    Args:\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be flipped.\n",
        "        Returns:\n",
        "            PIL Image: Randomly flipped image.\n",
        "        \"\"\"\n",
        "        if random.random() < self.p:\n",
        "            return torchvisionF.hflip(img), torchvisionF.hflip(lbl)\n",
        "        return img, lbl\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
        "\n",
        "class ExtToTensor(object):\n",
        "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
        "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "    def __init__(self, normalize=True, target_type='uint8'):\n",
        "        self.normalize = normalize\n",
        "        self.target_type = target_type\n",
        "    def __call__(self, pic, lbl):\n",
        "        \"\"\"\n",
        "        Note that labels will not be normalized to [0, 1].\n",
        "        Args:\n",
        "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
        "            lbl (PIL Image or numpy.ndarray): Label to be converted to tensor. \n",
        "        Returns:\n",
        "            Tensor: Converted image and label\n",
        "        \"\"\"\n",
        "        if self.normalize:\n",
        "            return torchvision.transforms.functional.to_tensor(pic), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "        else:\n",
        "            return torch.from_numpy( np.array( pic, dtype=np.float32).transpose(2, 0, 1) ), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'\n",
        "\n",
        "class ExtNormalize(object):\n",
        "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
        "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
        "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
        "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
        "    Args:\n",
        "        mean (sequence): Sequence of means for each channel.\n",
        "        std (sequence): Sequence of standard deviations for each channel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            tensor (Tensor): Tensor of label. A dummy input for ExtCompose\n",
        "        Returns:\n",
        "            Tensor: Normalized Tensor image.\n",
        "            Tensor: Unchanged Tensor label\n",
        "        \"\"\"\n",
        "        return torchvision.transforms.functional.normalize(tensor, self.mean, self.std), lbl\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "class ExtRandomCrop(object):\n",
        "    \"\"\"Crop the given PIL Image at a random location.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "        padding (int or sequence, optional): Optional padding on each border\n",
        "            of the image. Default is 0, i.e no padding. If a sequence of length\n",
        "            4 is provided, it is used to pad left, top, right, bottom borders\n",
        "            respectively.\n",
        "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
        "            desired size to avoid raising an exception.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, padding=0, pad_if_needed=False):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "        self.padding = padding\n",
        "        self.pad_if_needed = pad_if_needed\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img, output_size):\n",
        "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "            output_size (tuple): Expected output size of the crop.\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
        "        \"\"\"\n",
        "        w, h = img.size\n",
        "        th, tw = output_size\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "\n",
        "        i = random.randint(0, h - th)\n",
        "        j = random.randint(0, w - tw)\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __call__(self, img, lbl):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "            lbl (PIL Image): Label to be cropped.\n",
        "        Returns:\n",
        "            PIL Image: Cropped image.\n",
        "            PIL Image: Cropped label.\n",
        "        \"\"\"\n",
        "        assert img.size == lbl.size, 'size of img and lbl should be the same. %s, %s'%(img.size, lbl.size)\n",
        "        if self.padding > 0:\n",
        "            img = torchvisionF.pad(img, self.padding)\n",
        "            lbl = torchvisionF.pad(lbl, self.padding)\n",
        "\n",
        "        # pad the width if needed\n",
        "        if self.pad_if_needed and img.size[0] < self.size[1]:\n",
        "            img = torchvisionF.pad(img, padding=int((1 + self.size[1] - img.size[0]) / 2))\n",
        "            lbl = torchvisionF.pad(lbl, padding=int((1 + self.size[1] - lbl.size[0]) / 2))\n",
        "\n",
        "        # pad the height if needed\n",
        "        if self.pad_if_needed and img.size[1] < self.size[0]:\n",
        "            img = torchvisionF.pad(img, padding=int((1 + self.size[0] - img.size[1]) / 2))\n",
        "            lbl = torchvisionF.pad(lbl, padding=int((1 + self.size[0] - lbl.size[1]) / 2))\n",
        "\n",
        "        i, j, h, w = self.get_params(img, self.size)\n",
        "\n",
        "        return torchvisionF.crop(img, i, j, h, w), torchvisionF.crop(lbl, i, j, h, w)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Zogp0wa3nX"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "def get_dataset(opts):\r\n",
        "    \"\"\" Dataset And Augmentation\r\n",
        "    \"\"\"\r\n",
        "    if opts.dataset == 'voc':\r\n",
        "        train_transform = ExtCompose([\r\n",
        "            #et.ExtResize(size=opts.crop_size),\r\n",
        "            ExtRandomScale((0.5, 2.0)),\r\n",
        "            ExtRandomCrop(size=(opts.crop_size, opts.crop_size), pad_if_needed=True),\r\n",
        "            ExtRandomHorizontalFlip(),\r\n",
        "            ExtToTensor(),\r\n",
        "            ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                            std=[0.229, 0.224, 0.225]),\r\n",
        "        ])\r\n",
        "        if opts.crop_val:\r\n",
        "            val_transform = ExtCompose([\r\n",
        "                ExtResize(opts.crop_size),\r\n",
        "                ExtCenterCrop(opts.crop_size),\r\n",
        "                ExtToTensor(),\r\n",
        "                ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                                std=[0.229, 0.224, 0.225]),\r\n",
        "            ])\r\n",
        "        else:\r\n",
        "            val_transform = ExtCompose([\r\n",
        "                ExtToTensor(),\r\n",
        "                ExtNormalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                                std=[0.229, 0.224, 0.225]),\r\n",
        "            ])\r\n",
        "        train_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\r\n",
        "                                    image_set='train', transform=train_transform)\r\n",
        "        val_dst = VOCSegmentation(root=opts.data_root, year=opts.year,\r\n",
        "                                  image_set='val', transform=val_transform)\r\n",
        "    \r\n",
        "    return train_dst, val_dst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3paRH68cENLS"
      },
      "source": [
        "#4. Train/Test\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVkLeySIrfQU"
      },
      "source": [
        "### (1) poly learning rate policy\r\n",
        "\r\n",
        "- 초반엔 선형 감소에 가깝지만, Training step의 마지막에서는 조금 더 가파르게 감소하는 경향성을 나타냄\r\n",
        "$$learning\\_rate = \\left( 1-\\frac{iter}{max\\_iter}\\right)^{power}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F-BsVwkMyGS"
      },
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler, StepLR\r\n",
        "\r\n",
        "class PolyLR(_LRScheduler):\r\n",
        "    def __init__(self, optimizer, max_iters, power=0.9, last_epoch=-1, min_lr=1e-6):\r\n",
        "        self.power = power\r\n",
        "        self.max_iters = max_iters  # avoid zero lr\r\n",
        "        self.min_lr = min_lr\r\n",
        "        super(PolyLR, self).__init__(optimizer, last_epoch)\r\n",
        "    \r\n",
        "    def get_lr(self):\r\n",
        "        return [ max( base_lr * ( 1 - self.last_epoch/self.max_iters )**self.power, self.min_lr)\r\n",
        "                for base_lr in self.base_lrs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxOD7cMcrpVm"
      },
      "source": [
        "###(2) Focal Loss\r\n",
        "- ignore_index: 특정 target value를 무시하며, gradient 계산 할 때 포함시키지 않는다. size_average가 true일 경우 ignore_index되어 있는 target을 제외하고 평균을 계산한다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO6caE8yM1fy"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch \r\n",
        "\r\n",
        "class FocalLoss(nn.Module):\r\n",
        "    def __init__(self, alpha=1, gamma=0, size_average=True, ignore_index=255):\r\n",
        "        super(FocalLoss, self).__init__()\r\n",
        "        self.alpha = alpha\r\n",
        "        self.gamma = gamma\r\n",
        "        self.ignore_index = ignore_index\r\n",
        "        self.size_average = size_average\r\n",
        "\r\n",
        "    def forward(self, inputs, targets):\r\n",
        "        ce_loss = F.cross_entropy(\r\n",
        "            inputs, targets, reduction='none', ignore_index=self.ignore_index)\r\n",
        "        pt = torch.exp(-ce_loss)\r\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\r\n",
        "        if self.size_average:\r\n",
        "            return focal_loss.mean()\r\n",
        "        else:\r\n",
        "            return focal_loss.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY9cLh7vKE48"
      },
      "source": [
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "def set_bn_momentum(model, momentum=0.1):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            m.momentum = momentum\n",
        "\n",
        "def save_ckpt(path):\n",
        "    \"\"\" save current model\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "        \"cur_itrs\": cur_itrs,\n",
        "        \"model_state\": model.module.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_score\": best_score,\n",
        "    }, path)\n",
        "    print(\"Model saved as %s\" % path)\n",
        "\n",
        "class Denormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        mean = np.array(mean)\n",
        "        std = np.array(std)\n",
        "        self._mean = -mean/std\n",
        "        self._std = 1/std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        if isinstance(tensor, np.ndarray):\n",
        "            return (tensor - self._mean.reshape(-1,1,1)) / self._std.reshape(-1,1,1)\n",
        "        return normalize(tensor, self._mean, self._std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE4DxXTPizUV"
      },
      "source": [
        "###(3) Validation / Performance Measure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRTzZ96Ma6Yl"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import cv2\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def validate(opts, model, loader, device, metrics):\r\n",
        "    \"\"\"Do validation and return specified samples\"\"\"\r\n",
        "    metrics.reset()\r\n",
        "    ret_samples = []\r\n",
        "    if opts.save_val_results:\r\n",
        "        if not os.path.exists('results'):\r\n",
        "            os.mkdir('results')\r\n",
        "        denorm = Denormalize(mean=[0.485, 0.456, 0.406], \r\n",
        "                              std=[0.229, 0.224, 0.225])\r\n",
        "        img_id = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, (images, labels) in tqdm(enumerate(loader)):\r\n",
        "            \r\n",
        "            images = images.to(device, dtype=torch.float32)\r\n",
        "            labels = labels.to(device, dtype=torch.long)\r\n",
        "\r\n",
        "            outputs = model(images)\r\n",
        "            preds = outputs.detach().max(dim=1)[1].cpu().numpy()\r\n",
        "            targets = labels.cpu().numpy()\r\n",
        "\r\n",
        "            metrics.update(targets, preds)\r\n",
        "\r\n",
        "            if opts.save_val_results:\r\n",
        "                for i in range(len(images)):\r\n",
        "                    image = images[i].detach().cpu().numpy()\r\n",
        "                    target = targets[i]\r\n",
        "                    pred = preds[i]\r\n",
        "\r\n",
        "                    image = (denorm(image) * 255).transpose(1, 2, 0).astype(np.uint8)\r\n",
        "                    target = loader.dataset.decode_target(target).astype(np.uint8)\r\n",
        "                    pred = loader.dataset.decode_target(pred).astype(np.uint8)\r\n",
        "\r\n",
        "                    Image.fromarray(image).save('results/%d_image.png' % img_id)\r\n",
        "                    Image.fromarray(target).save('results/%d_target.png' % img_id)\r\n",
        "                    Image.fromarray(pred).save('results/%d_pred.png' % img_id)\r\n",
        "\r\n",
        "                    fig = plt.figure()\r\n",
        "                    plt.imshow(image)\r\n",
        "                    plt.axis('off')\r\n",
        "                    plt.imshow(pred, alpha=0.7)\r\n",
        "                    ax = plt.gca()\r\n",
        "                    ax.xaxis.set_major_locator(matplotlib.ticker.NullLocator())\r\n",
        "                    ax.yaxis.set_major_locator(matplotlib.ticker.NullLocator())\r\n",
        "                    plt.savefig('results/%d_overlay.png' % img_id, bbox_inches='tight', pad_inches=0)\r\n",
        "                    plt.close()\r\n",
        "\r\n",
        "\r\n",
        "                    if opts.test_only:\r\n",
        "                      imgBGR = cv2.imread('results/%d_image.png' % img_id)\r\n",
        "\r\n",
        "                      # cv2.imread는 BGR로 불러오므로 plt를 이용하려면 RGB로 바꿔줘야 함\r\n",
        "                      imgTargetBGR = cv2.imread('results/%d_target.png' % img_id)\r\n",
        "                      imgPredBGR = cv2.imread('results/%d_pred.png' % img_id)\r\n",
        "                      imgOverBGR = cv2.imread('results/%d_overlay.png' % img_id)\r\n",
        "                      plt.axis('off')\r\n",
        "\r\n",
        "                      plt.subplot(141), plt.axis('off'), plt.imshow(imgBGR)\r\n",
        "                      plt.subplot(142), plt.axis('off'), plt.imshow(imgTargetBGR)\r\n",
        "                      plt.subplot(143), plt.axis('off'), plt.imshow(imgPredBGR)\r\n",
        "                      plt.subplot(144), plt.axis('off'), plt.imshow(imgOverBGR)\r\n",
        "                      plt.show()\r\n",
        "\r\n",
        "\r\n",
        "                    img_id += 1\r\n",
        "\r\n",
        "        score = metrics.get_results()\r\n",
        "    return score, ret_samples\r\n",
        "\r\n",
        "class StreamSegMetrics(object):\r\n",
        "    \"\"\"\r\n",
        "    Stream Metrics for Semantic Segmentation Task\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, n_classes):\r\n",
        "        self.n_classes = n_classes\r\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\r\n",
        "\r\n",
        "    def update(self, label_trues, label_preds):\r\n",
        "        for lt, lp in zip(label_trues, label_preds):\r\n",
        "            self.confusion_matrix += self._fast_hist( lt.flatten(), lp.flatten() )\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def to_str(results):\r\n",
        "        string = \"\\n\"\r\n",
        "        for k, v in results.items():\r\n",
        "            if k!=\"Class IoU\":\r\n",
        "                string += \"%s: %f\\n\"%(k, v)\r\n",
        "        \r\n",
        "        #string+='Class IoU:\\n'\r\n",
        "        #for k, v in results['Class IoU'].items():\r\n",
        "        #    string += \"\\tclass %d: %f\\n\"%(k, v)\r\n",
        "        return string\r\n",
        "\r\n",
        "    def _fast_hist(self, label_true, label_pred):\r\n",
        "        mask = (label_true >= 0) & (label_true < self.n_classes)\r\n",
        "        hist = np.bincount(\r\n",
        "            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\r\n",
        "            minlength=self.n_classes ** 2,\r\n",
        "        ).reshape(self.n_classes, self.n_classes)\r\n",
        "        return hist\r\n",
        "\r\n",
        "    def get_results(self):\r\n",
        "        \"\"\"Returns accuracy score evaluation result.\r\n",
        "            - overall accuracy\r\n",
        "            - mean accuracy\r\n",
        "            - mean IU\r\n",
        "            - fwavacc\r\n",
        "        \"\"\"\r\n",
        "        hist = self.confusion_matrix\r\n",
        "        acc = np.diag(hist).sum() / hist.sum()\r\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\r\n",
        "        acc_cls = np.nanmean(acc_cls)\r\n",
        "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\r\n",
        "        mean_iu = np.nanmean(iu)\r\n",
        "        freq = hist.sum(axis=1) / hist.sum()\r\n",
        "        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\r\n",
        "        cls_iu = dict(zip(range(self.n_classes), iu))\r\n",
        "\r\n",
        "        return {\r\n",
        "                \"Overall Acc\": acc,\r\n",
        "                \"Mean Acc\": acc_cls,\r\n",
        "                \"FreqW Acc\": fwavacc,\r\n",
        "                \"Mean IoU\": mean_iu,\r\n",
        "                \"Class IoU\": cls_iu,\r\n",
        "            }\r\n",
        "        \r\n",
        "    def reset(self):\r\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPKZT2Wr_Th"
      },
      "source": [
        "##(4) Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApSLxQMiBCnm"
      },
      "source": [
        "#!wget -P checkpoints/ \"https://www.dropbox.com/s/3eag5ojccwiexkq/best_deeplabv3_resnet50_voc_os16.pth?dl=1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W4FAzZoeLGB"
      },
      "source": [
        "class Config():\r\n",
        "    # TODO : ADD compound scaling factor\r\n",
        "    def __init__(self):\r\n",
        "        super(Config, self).__init__()\r\n",
        "\r\n",
        "        self.data_root='./datasets/data'\r\n",
        "        self.dataset ='voc'\r\n",
        "        self.download =False\r\n",
        "        self.year ='2012' # VOC\r\n",
        "\r\n",
        "        self.num_classes = 21\r\n",
        "        self.model ='deeplabv3_resnet50_CARAFE'\r\n",
        "        self.output_stride =16\r\n",
        "\r\n",
        "        self.pretrained_backbone = True\r\n",
        "\r\n",
        "        # train option\r\n",
        "        self.test_only =False\r\n",
        "        self.save_val_results =True\r\n",
        "        self.total_itrs =30e3\r\n",
        "        self.lr =0.01\r\n",
        "        self.lr_policy ='poly' # 'step'\r\n",
        "        self.step_size =10000\r\n",
        "        self.crop_val =False\r\n",
        "        self.batch_size =16\r\n",
        "        self.val_batch_size =4\r\n",
        "        self.crop_size =513\r\n",
        "        self.ckpt ='checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth'\r\n",
        "        self.continue_training =False\r\n",
        "\r\n",
        "        self.loss_type = 'cross_entropy' #'cross_entropy', 'focal_loss'\r\n",
        "        self.gpu_id ='0'\r\n",
        "        self.weight_decay =1e-4\r\n",
        "        self.random_seed =1\r\n",
        "        self.print_interval =10\r\n",
        "        self.val_interval =100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6JORB0j-csjm",
        "outputId": "6c47b0b6-4e6b-400d-a767-48c4115973ec"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils import data\r\n",
        "\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "# Load option\r\n",
        "opts =Config()\r\n",
        "\r\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(\"Device: %s\" % device)\r\n",
        "\r\n",
        "# Setup random seed\r\n",
        "torch.manual_seed(opts.random_seed)\r\n",
        "np.random.seed(opts.random_seed)\r\n",
        "random.seed(opts.random_seed)\r\n",
        "\r\n",
        "\r\n",
        "# Set dataloader\r\n",
        "if opts.dataset.lower() == 'voc':\r\n",
        "    opts.num_classes = 21\r\n",
        "#elif opts.dataset.lower() == 'cityscapes':\r\n",
        "#    opts.num_classes = 19\r\n",
        "\r\n",
        "if opts.dataset=='voc' and not opts.crop_val:\r\n",
        "    opts.val_batch_size = 1\r\n",
        "\r\n",
        "train_dst, val_dst = get_dataset(opts)\r\n",
        "train_loader = data.DataLoader(\r\n",
        "    train_dst, batch_size=opts.batch_size, shuffle=True, num_workers=2)\r\n",
        "val_loader = data.DataLoader(\r\n",
        "    val_dst, batch_size=opts.val_batch_size, shuffle=True, num_workers=2)\r\n",
        "print(\"Dataset: %s, Train set: %d, Val set: %d\" %\r\n",
        "      (opts.dataset, len(train_dst), len(val_dst)))\r\n",
        "\r\n",
        "# Set up model\r\n",
        "model  = ResNetbasedDeepLabV3(num_classes=opts.num_classes, output_stride=opts.output_stride, pretrained_backbone=opts.pretrained_backbone)\r\n",
        "set_bn_momentum(model.backbone, momentum=0.01)\r\n",
        "\r\n",
        "# Set up optimizer\r\n",
        "optimizer = torch.optim.SGD(params=[\r\n",
        "    {'params': model.backbone.parameters(), 'lr': 0.1*opts.lr},\r\n",
        "    {'params': model.classifier.parameters(), 'lr': opts.lr},\r\n",
        "], lr=opts.lr, momentum=0.9, weight_decay=opts.weight_decay)\r\n",
        "\r\n",
        "if opts.lr_policy=='poly':\r\n",
        "    scheduler = PolyLR(optimizer, opts.total_itrs, power=0.9)\r\n",
        "elif opts.lr_policy=='step':\r\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opts.step_size, gamma=0.1)\r\n",
        "\r\n",
        "# Set up criterion\r\n",
        "if opts.loss_type == 'focal_loss':\r\n",
        "    criterion = FocalLoss(ignore_index=255, size_average=True)\r\n",
        "elif opts.loss_type == 'cross_entropy':\r\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')\r\n",
        "\r\n",
        "# Set up metrics\r\n",
        "metrics = StreamSegMetrics(opts.num_classes)\r\n",
        "\r\n",
        "checkPointPath = 'checkpoints'\r\n",
        "if not os.path.exists(checkPointPath):\r\n",
        "    os.mkdir(checkPointPath)\r\n",
        "# Restore\r\n",
        "best_score = 0.0\r\n",
        "cur_itrs = 0\r\n",
        "cur_epochs = 0\r\n",
        "\r\n",
        "if opts.ckpt is not None and os.path.isfile(opts.ckpt):\r\n",
        "    # https://github.com/VainF/DeepLabV3Plus-Pytorch/issues/8#issuecomment-605601402, @PytaichukBohdan\r\n",
        "    checkpoint = torch.load(opts.ckpt, map_location=torch.device('cuda'))\r\n",
        "    model.load_state_dict(checkpoint[\"model_state\"])\r\n",
        "    model = nn.DataParallel(model)\r\n",
        "    model.to(device)\r\n",
        "    if opts.continue_training:\r\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\r\n",
        "        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\r\n",
        "        cur_itrs = checkpoint[\"cur_itrs\"]\r\n",
        "        best_score = checkpoint['best_score']\r\n",
        "        print(\"Training state restored from %s\" % opts.ckpt)\r\n",
        "    print(\"Model restored from %s\" % opts.ckpt)\r\n",
        "    del checkpoint  # free memory\r\n",
        "else:\r\n",
        "    print(\"[!] Retrain\")\r\n",
        "    model = nn.DataParallel(model)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "denorm = Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # denormalization for ori images\r\n",
        "\r\n",
        "if opts.test_only:\r\n",
        "    model.eval()\r\n",
        "    val_score, ret_samples = validate(opts=opts, model=model, loader=val_loader, device=device, metrics=metrics)\r\n",
        "    print(metrics.to_str(val_score))\r\n",
        "\r\n",
        "else:\r\n",
        "  interval_loss = 0\r\n",
        "  while True: #cur_itrs < opts.total_itrs:\r\n",
        "      # =====  Train  =====\r\n",
        "      model.train()\r\n",
        "      cur_epochs += 1\r\n",
        "      for (images, labels) in train_loader:\r\n",
        "          cur_itrs += 1\r\n",
        "\r\n",
        "          images = images.to(device, dtype=torch.float32)\r\n",
        "          labels = labels.to(device, dtype=torch.long)\r\n",
        "\r\n",
        "          optimizer.zero_grad()\r\n",
        "          outputs = model(images)\r\n",
        "          loss = criterion(outputs, labels)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "\r\n",
        "          np_loss = loss.detach().cpu().numpy()\r\n",
        "          interval_loss += np_loss\r\n",
        "\r\n",
        "          if (cur_itrs) % 10 == 0:\r\n",
        "              interval_loss = interval_loss/10\r\n",
        "              print(\"Epoch %d, Itrs %d/%d, Loss=%f\" %\r\n",
        "                    (cur_epochs, cur_itrs, opts.total_itrs, interval_loss))\r\n",
        "              interval_loss = 0.0\r\n",
        "\r\n",
        "          if (cur_itrs) % opts.val_interval == 0:\r\n",
        "              save_ckpt('checkpoints/latest_%s_%s_os%d.pth' %\r\n",
        "                        (opts.model, opts.dataset, opts.output_stride))\r\n",
        "              \r\n",
        "              print(\"validation...\")\r\n",
        "              \r\n",
        "              model.eval()\r\n",
        "              val_score, ret_samples = validate(\r\n",
        "                  opts=opts, model=model, loader=val_loader, device=device, metrics=metrics)\r\n",
        "              print(metrics.to_str(val_score))\r\n",
        "              if val_score['Mean IoU'] > best_score:  # save best model\r\n",
        "                  best_score = val_score['Mean IoU']\r\n",
        "                  save_ckpt('checkpoints/best_%s_%s_os%d.pth' %\r\n",
        "                            (opts.model, opts.dataset,opts.output_stride))\r\n",
        "              \r\n",
        "              #vis.vis_scalar(\"[Val] Overall Acc\", cur_itrs, val_score['Overall Acc'])\r\n",
        "              #vis.vis_scalar(\"[Val] Mean IoU\", cur_itrs, val_score['Mean IoU'])\r\n",
        "              #vis.vis_table(\"[Val] Class IoU\", val_score['Class IoU'])\r\n",
        "\r\n",
        "              #for k, (img, target, lbl) in enumerate(ret_samples):\r\n",
        "              #    img = (denorm(img) * 255).astype(np.uint8)\r\n",
        "              #    target = train_dst.decode_target(target).transpose(2, 0, 1).astype(np.uint8)\r\n",
        "              #    lbl = train_dst.decode_target(lbl).transpose(2, 0, 1).astype(np.uint8)\r\n",
        "              #    concat_img = np.concatenate((img, target, lbl), axis=2)  # concat along width\r\n",
        "              #    vis.vis_image('Sample %d' % k, concat_img)\r\n",
        "\r\n",
        "              model.train()\r\n",
        "          scheduler.step()  \r\n",
        "\r\n",
        "          #if cur_itrs >=  opts.total_itrs:\r\n",
        "              #return\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Dataset: voc, Train set: 1464, Val set: 1449\n",
            "Model restored from checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Itrs 10/30000, Loss=0.100845\n",
            "Epoch 1, Itrs 20/30000, Loss=0.092625\n",
            "Epoch 1, Itrs 30/30000, Loss=0.093359\n",
            "Epoch 1, Itrs 40/30000, Loss=0.097646\n",
            "Epoch 1, Itrs 50/30000, Loss=0.089101\n",
            "Epoch 1, Itrs 60/30000, Loss=0.089750\n",
            "Epoch 1, Itrs 70/30000, Loss=0.084165\n",
            "Epoch 1, Itrs 80/30000, Loss=0.090632\n",
            "Epoch 1, Itrs 90/30000, Loss=0.101166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, Itrs 100/30000, Loss=0.085282\n",
            "Model saved as checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth\n",
            "validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1449it [32:54,  1.36s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Acc: 0.879841\n",
            "Mean Acc: 0.745281\n",
            "FreqW Acc: 0.798868\n",
            "Mean IoU: 0.572428\n",
            "\n",
            "Model saved as checkpoints/best_deeplabv3_resnet50_CARAFE_voc_os16.pth\n",
            "Epoch 2, Itrs 110/30000, Loss=0.127214\n",
            "Epoch 2, Itrs 120/30000, Loss=0.116897\n",
            "Epoch 2, Itrs 130/30000, Loss=0.127013\n",
            "Epoch 2, Itrs 140/30000, Loss=0.122655\n",
            "Epoch 2, Itrs 150/30000, Loss=0.125529\n",
            "Epoch 2, Itrs 160/30000, Loss=0.120137\n",
            "Epoch 2, Itrs 170/30000, Loss=0.107338\n",
            "Epoch 2, Itrs 180/30000, Loss=0.121240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3, Itrs 190/30000, Loss=0.118151\n",
            "Epoch 3, Itrs 200/30000, Loss=0.131133\n",
            "Model saved as checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth\n",
            "validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1449it [07:38,  3.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Acc: 0.861704\n",
            "Mean Acc: 0.789893\n",
            "FreqW Acc: 0.777186\n",
            "Mean IoU: 0.549581\n",
            "\n",
            "Epoch 3, Itrs 210/30000, Loss=0.131211\n",
            "Epoch 3, Itrs 220/30000, Loss=0.130532\n",
            "Epoch 3, Itrs 230/30000, Loss=0.139765\n",
            "Epoch 3, Itrs 240/30000, Loss=0.150003\n",
            "Epoch 3, Itrs 250/30000, Loss=0.139473\n",
            "Epoch 3, Itrs 260/30000, Loss=0.135450\n",
            "Epoch 3, Itrs 270/30000, Loss=0.151406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py:365: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4, Itrs 280/30000, Loss=0.126718\n",
            "Epoch 4, Itrs 290/30000, Loss=0.151679\n",
            "Epoch 4, Itrs 300/30000, Loss=0.135294\n",
            "Model saved as checkpoints/latest_deeplabv3_resnet50_CARAFE_voc_os16.pth\n",
            "validation...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1190it [1:14:34, 895.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6c80a2d4b0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m               val_score, ret_samples = validate(\n\u001b[0;32m--> 133\u001b[0;31m                   opts=opts, model=model, loader=val_loader, device=device, metrics=metrics)\n\u001b[0m\u001b[1;32m    134\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mval_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mean IoU'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# save best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-27a000b2afdb>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(opts, model, loader, device, metrics)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1027) is killed by signal: Killed. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5laQupi1QImA"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}